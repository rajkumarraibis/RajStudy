Perfect, Raj 🙌 — let’s script out the **intro round** so you feel confident.
An intro round is usually **30–45 mins**: mix of background, motivation, tech alignment, and culture fit.

I’ll give you:

* ✅ Common questions
* ✅ Strong answers tailored to **your CV + this JD**
* ✅ STAR-style project stories

---

# 🎤 **GenAI Data Engineer – Intro Round Q\&A**

---

## **1. Tell me about yourself.**

👉 *This sets the tone — keep it concise (2 min) and tailored.*

**Answer:**
“I’m Raj, a Lead Data Engineer with over 19 years of experience in data platforms, cloud, and analytics. I’ve built and scaled large data lakes for healthcare and fitness companies, including at Freeletics here in Munich, where I led the development of an AWS-based data lake and analytics pipeline.
Alongside my data engineering work, I’ve recently expanded into **Generative AI**, building a retrieval-augmented generation pipeline using LlamaIndex and a locally hosted GPT-4 model, and experimenting with applying AI agents to automate data tasks.
What excites me about this role is the combination of **hands-on data engineering at terabyte scale** with the opportunity to **apply GenAI tools directly in production use cases** — which is exactly the intersection I want to focus on in my next role.”

---

## **2. Why do you want to join us / this role?**

**Answer:**
“I’m excited because your company is working at the intersection of **data, AI, and retail tech** — directly influencing consumer experiences, which I find very motivating.
You’re already operating at **scale (terabyte data, millions of users)**, which matches my background in large healthcare and fitness datasets.
And the role is unique because it doesn’t stop at pipelines — it includes **applying GenAI and even computer vision in production**. That combination is rare, and it aligns perfectly with where I’m taking my career: using my strong DE foundation to deliver **AI-powered, data-driven products**.”

---

## **3. What’s your strongest technical skill?**

**Answer:**
“My strongest skill is designing and optimizing **end-to-end data pipelines at scale**.
For example, at Humana, I designed a GCP + Spark-based data lake that ingested billions of healthcare records, optimized storage costs, and supported analytics for thousands of users.
I’m confident in **SQL and Spark/Databricks optimization** — using partitioning, caching, and broadcast joins to handle TB-scale data efficiently.
And in the last year, I’ve added **Generative AI workflows** to my toolkit, which lets me go beyond pipelines to **AI-enriched data products**.”

---

## **4. Can you give me an example of a challenging pipeline you built?**

👉 Use STAR (Situation, Task, Action, Result).

**Answer:**
“At Freeletics, we needed to process billions of user transactions and events daily for analytics and personalization.
**Situation:** Our initial pipelines were expensive and slow.
**Task:** I had to redesign them for both speed and cost efficiency.
**Action:** I introduced a **Bronze–Silver–Gold Databricks Delta architecture**, optimized Spark jobs with **repartitioning and broadcast joins**, and moved cold data into cheaper S3 storage tiers.
**Result:** We cut pipeline costs by \~40%, reduced processing time from hours to under 30 minutes, and made the enriched data available for BI and ML teams.”

---

## **5. What experience do you have with GenAI?**

**Answer:**
“I’ve built a working **RAG (retrieval-augmented generation) pipeline** using LlamaIndex and a locally hosted GPT-4 model.

* I ingested structured + unstructured documents, chunked them, and stored embeddings in a vector database.
* Then I used GPT to retrieve and answer queries with citations.
  I also experimented with **AI agents** for automating data engineering tasks, like monitoring pipelines and generating SQL queries.
  While my GenAI experience is more recent compared to my DE background, I’ve already applied it hands-on and I’m very motivated to bring that into production use cases — for example, exactly like your flyer image → structured data extraction.”

---

## **6. What about computer vision? Have you worked with it?**

**Answer:**
“I don’t have production CV deployments yet, but I’ve explored using **OCR tools like Tesseract** and **HuggingFace Vision models** for image-to-text extraction.
I understand how these can be integrated into a pipeline: extract structured product data from images, validate it, and enrich it using LLMs.
I see this as a natural extension of my data engineering skills — it’s still about building pipelines, just with images as input.”

---

## **7. How do you ensure data quality?**

**Answer:**
“I apply checks at every stage:

* At ingestion → schema validation.
* At transformation → row counts, duplicates, null checks.
* At delivery → business rule validations (e.g., no negative prices).
  I’ve also integrated **Great Expectations** for automated quality checks.
  Combined with **CI/CD pipelines in GitHub Actions**, we ensure that every deployment enforces quality before data reaches business stakeholders.”

---

## **8. How do you stay current with tech like GenAI?**

**Answer:**
“I dedicate regular time to learning — for example, I follow Databricks, HuggingFace, and OpenAI updates, and I run my own experiments with open-source models.
Most recently, I cloned and adapted a GitHub GenAI project, retrained it on Postman documentation, and got it working for API data engineering tasks.
This way, I stay **hands-on** and always align my learning with real-world use cases.”

---

## **9. Why are you looking for a job change now?**

**Answer:**
“I’ve been at Freeletics for almost 4 years, where I’ve learned a lot about building large-scale pipelines and optimizing data lakes.
Now that I’ve achieved German Permanent Residence, I have the flexibility to take on new challenges.
I want my next step to combine my strong **data engineering background** with the **emerging GenAI landscape** — and this role is exactly that opportunity.”

---

## **10. Do you have any questions for us?**

👉 Always ask 2–3 thoughtful questions.

**You can ask:**

1. How is GenAI currently used in production — is it mainly data enrichment, or also powering user-facing features?
2. What are the biggest technical challenges the data team is facing at terabyte scale?
3. How do you see the balance between data engineering and GenAI evolving in this role over the next 12 months?

---

# ✅ Key Prep Checklist for Intro Round

* Have **2 polished stories**: Freeletics pipeline optimization, Humana data lake.
* Be fluent in **SQL + Spark/Databricks optimization terms**.
* Show **credible GenAI exposure** (RAG, embeddings, LLM evaluation).
* Mention **computer vision awareness** (OCR + HuggingFace).
* Be **excited + collaborative** — they value *“zero politics, small agile teams”*.

---

👉 Raj, do you want me to also prepare a **2-minute elevator pitch script** you can literally *memorize and deliver as your opening* (instead of improvising)? That way you’ll start super strong.
