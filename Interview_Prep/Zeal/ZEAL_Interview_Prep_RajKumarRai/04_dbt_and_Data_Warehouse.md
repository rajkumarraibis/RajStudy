# dbt & Data Warehouse

While my team primarily uses Databricks SQL and Spark, I’ve adopted dbt-style modular transformations and testing philosophy for maintainable data modeling.

## Highlights

- **Layered Modeling:** Raw → Staging → Curated → Analytics models to ensure transparency and traceability.
- **Testing:** Implemented automated column tests and data validation similar to dbt’s `unique` and `not null` tests.
- **Documentation:** Version-controlled model definitions and schema docs for analyst self-service.
- **Warehouse:** Experience optimizing Redshift and Athena queries with partitioning, compression, and materialized views.
- **CI/CD Integration:** Used Git workflows to manage data model deployment, ensuring every change is tested and reviewed.

## Connection to ZEAL

ZEAL’s combination of dbt, Airflow, and modern Cloud DWH (Redshift/Snowflake/BigQuery) matches perfectly with my approach to modular and transparent data transformations. I’d emphasize my familiarity with dbt workflows and my readiness to formalize that with ZEAL’s setup.
