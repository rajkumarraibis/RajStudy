

##  Yannick Priority Pitch (Opening Statement)

“Thanks for having me. Let me quickly share how I see myself adding value at cosnova:

Data Modeling Expertise – I’ve worked with both Star Schema for analytics and Data Vault 2.0 for raw, auditable layers. I know how to combine them so the business gets fast insights, while IT keeps full history and compliance.
“I see Data Vault 2.0 as the foundation layer for compliance, history, and evolution, and then expose Star Schema marts for consumption. That way we balance governance with usability.”

Data Platform Ownership – At Freeletics and Humana, I built end-to-end platforms — ingestion, storage, transformation, and delivery — supporting both batch and streaming data. I see this role as owning the backbone that powers analytics and AI across cosnova.

Scalability & Optimization – I specialize in Databricks/Spark optimization: reducing shuffles, using broadcast joins, partition pruning, and solving the small-file problem. These optimizations directly translate into fresher data and lower costs.

Governance & Trust – I always embed data quality checks, lineage, and CI/CD into pipelines. In my view, governance isn’t a blocker — it’s what makes data truly usable and trustworthy for BI and AI teams.

Business Enablement – Ultimately, my goal is to make data a product that empowers stakeholders. For example, at Freeletics I cut pipeline runtimes from hours to under 30 minutes, which allowed product and AI teams to iterate much faster.”

“…That’s the value I’d like to bring to cosnova.”

---

## 2️⃣ Interview Simulation – Q\&A (as Yannick)

### 🔹 Q1: *“How would you design a scalable data platform for cosnova that supports analytics and AI use cases?”*

**✅ Ideal Answer:**

* Use a **Lakehouse architecture (Databricks Delta Lake on Azure)** → combines flexibility of Data Lake with reliability of DW.
* **Layers:** Bronze (raw), Silver (cleansed), Gold (business-ready).
* **Modeling:** Raw layer in **Data Vault 2.0** for history + audit; Marts in **Star Schema** for BI/analytics.
* **Governance:** Schema validation, Great Expectations, data lineage (Unity Catalog / Purview).
* **Access:** Expose via APIs, SQL endpoints, PowerBI/Snowflake for analysts.

**🧠 Reasoning:**

* Balance raw compliance needs (Data Vault) with usability (Star Schema).
* Lakehouse avoids siloed DW vs Lake.

**⚖️ Pros/Cons:**

* Pros: unified, scalable, cheaper.
* Cons: needs strong governance to avoid data swamp.

**📈 Business Value:**

* Gives **single source of truth** for analytics, speeds AI feature development, ensures **data quality + trust**.

---

### 🔹 Q2: *“What’s your view on Data Vault 2.0 vs Star Schema? When would you use which?”*

**✅ Ideal Answer:**

* **Data Vault 2.0** → raw, auditable, historical layer (great for compliance-heavy or evolving sources).
* **Star Schema** → simplified, business-facing marts (great for BI/reporting).
* **Strategy:** use DV 2.0 in raw/enterprise layer, then transform into Star for consumption.

**🧠 Reasoning:**

* DV 2.0 handles schema evolution + history; Star Schema optimizes query performance.

**📈 Business Value:**

* Analysts get simple tables, but IT retains auditability.

---

### 🔹 Q3: *“How do you optimize Spark/Databricks pipelines at TB scale?”*

**✅ Ideal Answer:**

* Minimize **shuffles** (narrow vs wide transformations).
* Use **broadcast joins** for small lookup tables.
* **Partition pruning** in Delta (by date, id).
* Avoid **small-file problem** → coalesce + OPTIMIZE.
* Cache intermediate datasets for iterative jobs.

**📈 Business Value:**

* Faster jobs → fresher insights, lower cloud costs.

---

### 🔹 Q4: *“How do you ensure data quality and governance in pipelines?”*

**✅ Ideal Answer:**

* Validation at ingestion (schema, null checks).
* Transformation checks (row counts, duplicates).
* Great Expectations / Deequ for automated data tests.
* Lineage via Unity Catalog / Purview.
* CI/CD enforcement in GitHub Actions.

**📈 Business Value:**

* Builds **trust in data** → critical for adoption by BI + AI teams.

---

### 🔹 Q5: *“Tell me about a conflict with stakeholders and how you resolved it.”*

**✅ Ideal STAR Answer:**

* **Situation:** At Freeletics, product wanted features faster than pipelines allowed.
* **Task:** Balance speed with quality.
* **Action:** Built a phased delivery — quick interim dataset for product, while building robust Gold tables in parallel.
* **Result:** Product team unblocked, long-term governance intact.

**📈 Business Value:**

* Shows you balance **stakeholder urgency with platform integrity**.

---

---

## 3️⃣ Key Topics to Revise

* **SQL**: advanced joins, window functions, query tuning, CTEs.
* **Spark/Databricks**: partitioning, broadcast joins, caching, Delta Lake.
* **Data Modeling**: Data Vault 2.0 (hubs, links, satellites), Star Schema, Mesh vs Fabric.
* **Cloud (Azure)**: ADLS, Synapse, Databricks.
* **Data Governance**: Great Expectations, lineage tools.
* **Streaming**: Kafka/EventHub → Databricks Structured Streaming.
* **CI/CD**: containerization, GitHub Actions for pipelines.

---

## 4️⃣ Common Pitfalls (Avoid These)

* Over-focusing on coding (they want **architecture vision**).
* Treating Data Vault as a product (it’s a methodology).
* Ignoring governance → they want trustable data.
* Over-engineering → show you balance **simplicity vs complexity**.
* Forgetting **business value** → always link tech to impact.

---

## 5️⃣ Quick Cheat Sheet

* **Data Vault 2.0:** hubs, links, satellites → raw, historical, auditable.
* **Star Schema:** facts + dimensions → analytics-friendly.
* **Data Mesh:** org paradigm, domain-owned data products.
* **Data Fabric:** vendor-driven unified metadata/governance.
* **Spark optimization buzzwords:** minimize shuffles, broadcast join, partition pruning, coalesce small files, cache(), Z-Order.
* **Governance buzzwords:** Great Expectations, lineage, Unity Catalog, Purview.

---

## 6️⃣ STAR Behavioral Questions (Sample Answers for You)

1. **Challenge in scaling pipelines?** → Freeletics optimization (40% cost cut, 4h → 30m runtime).
2. **Cross-team collaboration?** → Built shared Gold layer across Data Science & BI.
3. **Dealing with ambiguity?** → Humana: multiple siloed sources, created standardized data lake.
4. **Conflict resolution?** → Prioritized stakeholder needs with phased delivery.
5. **Leadership impact?** → Mentored juniors in Spark, led to faster onboarding.
6. **Innovation?** → Experimented with RAG pipeline using LlamaIndex → future-ready.
7. **Failure story?** → First schema migration failed due to missing lineage; implemented governance checks.

---

## 7️⃣ Wrap-up

### **Key Strengths to Highlight**

* 19y experience across **data lakes, cloud, DE leadership**.
* Strong in **SQL + Spark/Databricks**.
* Familiar with **Data Vault 2.0 + Star Schema**.
* Proven history in **governance + cost optimization**.
* Early **hands-on GenAI exposure** (future-proof).

### **Red Flags to Avoid**

* Don’t say *“I’ve never worked in Azure”* → instead, say *“I’ve worked in AWS/GCP, and Databricks concepts transfer directly to Azure.”*
* Don’t overpromise GenAI skills — keep focus on DE foundation.
* Avoid jargon without business link.

### **Closing Statements/Questions**

* *“How do you see the role of Data Vault 2.0 evolving in your data strategy over the next 2 years?”*
* *“What’s the biggest challenge your data team faces with scaling analytics?”*
* *“How do you balance building a central platform vs empowering domain teams (mesh-style)?”*
* *“Where do you see opportunities for GenAI in cosnova’s data platform?”*
* *“What does success look like in the first 6 months for this role?”*

---

✅ With this, you’ll walk into the pre-round sounding like a **senior data platform owner who understands both tech + business value.**

---

👉 Raj, do you also want me to generate a **mock Q\&A transcript** (like a roleplay where Yannick asks you questions and you answer) so you can *practice speaking the answers* before Monday?
